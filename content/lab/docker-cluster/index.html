---
author: brianomeara_ra6153
comments: false
date: 2017-12-01 14:48:31+00:00
layout: page
link: http://www.brianomeara.info/lab/docker-cluster/
slug: docker-cluster
title: Docker cluster
wordpress_id: 929
---



<p>After futzing with various cluster approaches (HTCondor, Slurm, other approaches) I’ve settled (for now) on using <code>foreach</code> with <code>doRedis</code> talking to worker nodes running Docker instances.
<a href="https://www.docker.com/what-docker">Docker</a> is like a lightweight virtual machine that can run on another computer – basically, a computer within a computer (though not as slow as this sounds). A container can store everything needed to, say, run R, and there are many, many containers. One advantage is rather than having to worry about all the dependencies, installing all the software, you can just type <code>docker run</code> followed by the container name and start it going. For example, I maintain a <a href="https://github.com/bomeara/phydocker">phylogenetics container</a>: after installing docker, you can go to the command line and type
<code>docker run -it --name phydocker -v /Path/To/My/Folder:/data -p 8787:8787 bomeara/phydocker</code>
replacing <code>/Path/To/My/Folder</code> with the actual path you want to use (i.e., <code>/Users/brianomeara/Desktop</code>) and then you can go to <a href="http://localhost:8787" class="uri">http://localhost:8787</a>, login as user and password <code>rstudio</code>, and use an RStudio instance that has a bunch of phylogenetics packages already installed, and which has access to your folder (in my example above, my desktop) as the /data folder in the container (so you can save to it and it’ll be saved on your actual Desktop). You can run multiple containers at once: download one that has python scripts you need, etc.
I’m using this for our cluster, too. Rather than try to synchronize R versions, packages, etc. across a heterogeneous set of computers, I have a container that has a stable version of R and recent packages, and we can just add more to that when we need to and redeploy. It can also work across architectures: if someone has a big Linux or Windows machine, we could use that with our Macs seamlessly.
For parallelization, we’re using the <a href="https://cran.r-project.org/web/packages/foreach/index.html">foreach</a> package. It’s well-documented, frequently updated, and popular (but see the <a href="https://cran.r-project.org/web/views/HighPerformanceComputing.html">CRAN High Performance Computing</a> task view for alternatives). Redis is a lightweight database; we’re using that (an instance of it running on a server) to keep track of submitted jobs and worker nodes using the <a href="https://github.com/bwlewis/doRedis">doRedis</a> package (the github version, which fixes a few bugs). There are other ways to handle this: real HPC software does stuff like prioritize jobs (person A, you ran a lot, so any job person B submits cuts ahead of yours in the queue) but we don’t seem to have enough usage to make this worth the complexity (both in installation, which I failed at, and in submitting jobs).</p>
<div id="file-syncing" class="section level2">
<h2>File syncing</h2>
<p>We use Unison to synchronize files. For those on a Mac, you can install homebrew, then <code>brew install unison</code> . Make sure you deposit your public key on <code>~/.ssh/authorized_keys</code> on 13 (ask in person).
Then (replacing SERVER_URL with the URL of our server; keep the double slashes)
<code>mkdir /Users/Shared/cluster</code> on your computer
<code>unison -testServer /Users/Shared/cluster ssh://SERVER_URL//Users/Shared/cluster</code>
<code>unison -auto -batch /Users/Shared/cluster ssh://SERVER_URL//Users/Shared/cluster</code>
Then do <code>crontab -e</code> and then add a line with
<code>*/3 * * * * /usr/local/bin/unison -auto -batch /Users/Shared/cluster ssh://SERVER_URL//Users/Shared/cluster</code></p>
</div>
<div id="job-submission" class="section level2">
<h2>Job submission</h2>
<p>To submit a job, run <code>docker run -it -v /Users/Shared/cluster:/cluster bomeara/omearaclusterworker /bin/bash</code> (you’ll need to download this first; I’ll show you how, as I don’t want it public), or on a mac you could just do <code>source(" /Users/Shared/cluster/redis/RedisScriptMac.R")</code> from within R.</p>
</div>
<div id="using-foreach" class="section level2">
<h2>Using foreach</h2>
<p>See the documentation</p>
<ul>
<li><p>Using foreach <a href="https://cran.r-project.org/web/packages/foreach/vignettes/foreach.pdf">vignette</a></p></li>
<li><p><a href="https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf">Using doParallel and foreach</a></p></li>
<li><p> <a href="https://cran.r-project.org/web/packages/foreach/vignettes/nested.pdf">Nesting foreach loops</a></p></li>
<li><p><a href="https://cran.r-project.org/web/packages/doRedis/vignettes/doRedis.pdf">doRedis vignette</a></p></li>
</ul>
<p>Note that as with most parallelization, you want to parallelize at the highest level possible. For example, take bootstrapping for likelihood. You make a new dataset from the old, and for each dataset, propose a set of parameter values (including topology), calculate the likelihood for each site, add these across sites, try a new parameter value, repeat. One way to parallelize would be every time the likelihood is calculated on a site, do this on a different machine for every site (really, site pattern, but that’s quibbling). That’s the lowest level possible – sending lots of really tiny jobs out. The highest level would be sending out each bootstrap replicate as a different job. Basically, you want each node to be churning away on calculations, only talking back to the manager rarely. One can go too extreme – send jobs off in months-long chunks, that have to be restarted when a computer inevitably shuts down.
An example run is
<code>foreach(j=1:17,.combine=paste, .multicombine=TRUE) %dopar% paste(system("hostname", intern=TRUE), system("whoami", intern=TRUE), Sys.getpid(), unname(system.time(mean(runif(1e6)))[3]))</code>
Oh, one note about jargon: 13 is the manager, and the other machines are workers. Some documentation online talks about masters and slaves, which is pretty disgusting.</p>
</div>
